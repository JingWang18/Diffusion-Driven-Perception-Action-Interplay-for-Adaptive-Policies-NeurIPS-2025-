<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Act to See, See to Act: Diffusion-Driven Perception–Action Interplay for Adaptive Policies">
  <meta name="description" content="Official project page for DP-AG, a diffusion-driven perception–action framework that delivers adaptive, robust robotic manipulation and outperforms prior diffusion policies.">
  <meta name="keywords" content="DP-AG, diffusion policy, perception-action loop, robotic manipulation, imitation learning, latent dynamics, NeurIPS 2025">
  <meta name="author" content="Jing Wang¹, Weiting Peng², Jing Tang², Zeyu Gong², Xihua Wang¹, Bo Tao², and Li Cheng¹">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="1. University of Alberta & 2. Huazhong University of Science and Technology">
  <meta property="og:title" content="Act to See, See to Act: Diffusion-Driven Perception–Action Interplay for Adaptive Policies">
  <meta property="og:description" content="DP-AG tightly couples perception and action with diffusion guidance to yield adaptive, robust manipulation policies.">
  <meta property="og:url" content="https://dp-ag.github.io/">
  <meta property="og:image" content="https://dp-ag.github.io/static/images/preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <!-- <meta property="og:image:alt" content=""> -->
  <!-- <meta property="article:published_time" content="2024-01-01T00:00:00.000Z"> -->
  <!-- <meta property="article:author" content="Jing Wang"> -->
  <meta property="article:section" content="Research">
  <!-- <meta property="article:tag" content="KEYWORD1"> -->
  <!-- <meta property="article:tag" content="KEYWORD2"> -->

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Act to See, See to Act: Diffusion-Driven Perception–Action Interplay for Adaptive Policies">
  <meta name="twitter:description" content="DP-AG couples perception and action via diffusion guidance to deliver adaptive robotic manipulation.">
  <meta name="twitter:image" content="https://dp-ag.github.io/static/images/preview.png">
  <meta name="twitter:image:alt" content="DP-AG project teaser">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Act to See, See to Act: Diffusion-Driven Perception–Action Interplay for Adaptive Policies">
  <meta name="citation_author" content="Wang, Jing">
  <meta name="citation_author" content="Peng, Weiting">
  <meta name="citation_author" content="Tang, Jing">
  <meta name="citation_author" content="Gong, Zeyu">
  <meta name="citation_author" content="Wang, Xihua">
  <meta name="citation_author" content="Tao, Bo">
  <meta name="citation_author" content="Cheng, Li">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="Advances in Neural Information Processing Systems (NeurIPS)">
  <meta name="citation_pdf_url" content="https://dp-ag.github.io/static/pdfs/sample.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>Act to See, See to Act: Diffusion-Driven Perception–Action Interplay for Adaptive Policies</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Act to See, See to Act: Diffusion-Driven Perception–Action Interplay for Adaptive Policies",
    "description": "DP-AG unifies perception and action through diffusion guidance, delivering adaptive, robust manipulation policies that surpass prior diffusion approaches.",
    "author": [
      {
        "@type": "Person",
        "name": "Jing Wang¹",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Alberta"
        }
      },
      {
        "@type": "Person",
        "name": "Weiting Peng²",
        "affiliation": {
          "@type": "Organization",
          "name": "Huazhong University of Science and Technology"
        }
      },
      {
        "@type": "Person",
        "name": "Jing Tang²",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Alberta"
        }
      },
      {
        "@type": "Person",
        "name": "Zeyu Gong²",
        "affiliation": {
          "@type": "Organization",
          "name": "Huazhong University of Science and Technology"
        }
      },
      {
        "@type": "Person",
        "name": "Xihua Wang¹",
        "affiliation": {
          "@type": "Organization",
          "name": "Huazhong University of Science and Technology"
        }
      },
      {
        "@type": "Person",
        "name": "Bo Tao²",
        "affiliation": {
          "@type": "Organization",
          "name": "Huazhong University of Science and Technology"
        }
      },
      {
        "@type": "Person",
        "name": "Li Cheng¹",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Alberta"
        }
      }
    ],
    "datePublished": "2025",
    "publisher": {
      "@type": "Organization",
      "name": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    "image": "https://dp-ag.github.io/static/images/carousel1.jpg",
    "keywords": ["DP-AG", "diffusion policy", "perception-action", "robotic manipulation", "imitation learning", "NeurIPS 2025"],
    "abstract": "Diffusion Policy with Action Guidance (DP-AG) jointly reasons over latent perception and action trajectories, enabling adaptive manipulation that surpasses prior diffusion-policy baselines in simulation and on hardware.",
    "citation": "@inproceedings{wang2025act, title={Act to See, See to Act: Diffusion-Driven Perception--Action Interplay for Adaptive Policies}, author={Wang, Jing and Peng, Weiting and Tang, Jing and Gong, Zeyu and Wang, Xihua and Tao, Bo and Cheng, Li}, booktitle={Advances in Neural Information Processing Systems (NeurIPS)}, year={2025}}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by-sa/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://dp-ag.github.io/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Robotic manipulation"
      },
      {
        "@type": "Thing",
        "name": "Diffusion models"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "1. University of Alberta & 2. Huazhong University of Science and Technology",
    "url": "https://dp-ag.github.io/",
    "logo": "https://dp-ag.github.io/static/images/favicon.ico",
    "sameAs": [
      "https://www.ualberta.ca/",
      "https://english.hust.edu.cn/"
    ]
  }
  </script>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Act to See, See to Act: Diffusion-Driven Perception–Action Interplay for Adaptive Policies</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Jing Wang¹</span>,
              <span class="author-block">Weiting Peng²</span>,
              <span class="author-block">Jing Tang²</span>,
              <span class="author-block">Zeyu Gong²</span>,
              <span class="author-block">Xihua Wang¹</span>,
              <span class="author-block">Bo Tao²</span>,
              <span class="author-block">Li Cheng¹</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">1. University of Alberta & 2. Huazhong University of Science and Technology</span><br>
              <span class="author-block">NeurIPS 2025</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://github.com/dp-ag/dp-ag.github.io#readme" target="_blank" rel="noopener"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-book-open"></i>
                    </span>
                    <span>Project Overview</span>
                  </a>
                </span>

                <span class="link-block">
                  <span class="external-link button is-normal is-rounded is-dark is-static" role="button" aria-disabled="true">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper (coming soon)</span>
                  </span>
                </span>

                <span class="link-block">
                  <a href="static/pdfs/sample.pdf" target="_blank" rel="noopener"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-chalkboard"></i>
                    </span>
                    <span>Poster Preview</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="video-container" style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden;">
        <iframe 
          src="https://www.youtube.com/embed/2Qa7ueFiDk0" 
          title="YouTube video player" 
          frameborder="0" 
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
          allowfullscreen
          style="position:absolute; top:0; left:0; width:100%; height:100%;">
        </iframe>
      </div>
      <h2 class="subtitle has-text-centered">
        DP-AG drives a closed perception–action loop: diffusion-guided action proposals inform perceptual updates on-the-fly, enabling robots to adapt smoothly to dynamic scenes and contact-rich manipulation.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
			Existing imitation learning methods decouple perception and action, which overlooks the causal reciprocity between sensory representations and action execution that humans naturally leverage for adaptive behaviors. To bridge this gap, we introduce Action-Guided Diffusion Policy (DP-AG), a unified representation learning that explicitly models a dynamic interplay between perception and action through probabilistic latent dynamics. DP-AG encodes latent observations into a Gaussian posterior via variational inference and evolves them using an action-guided SDE, where the Vector–Jacobian Product (VJP) of the diffusion policy's noise predictions serves as a structured stochastic force driving latent updates. To promote bidirectional learning between perception and action, we introduce a cycle-consistent contrastive loss that organizes the gradient flow of the noise predictor into a coherent perception–action loop, enforcing mutually consistent transitions in both latent updates and action refinements. Theoretically, we derive a variational lower bound for the action-guided SDE, and prove that the contrastive objective enhances continuity in both latent and action trajectories. Empirically, DP-AG significantly outperforms state-of-the-art methods across simulation benchmarks and real-world UR5 manipulation tasks. As a result, our DP-AG offers a promising step toward bridging biological adaptability and artificial policy learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="First research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Action-guided diffusion keeps perception aligned with executed motions, tightening the closed-loop policy.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          DP-AG boosts success rates by up to 13% on dynamic Push-T tasks over the best prior diffusion policy.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Robust latent rollouts preserve contact-rich manipulation sequences without mode collapse.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        Real-world experiments report 23% higher task completion rates with smoother gripper motions.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Video row -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Real-World Evaluations on UR5 Robot Arm</h2>
      <div class="real-world-video-row">

        <div class="real-world-video-card has-text-centered">
          <video poster="" id="video1" controls muted loop preload="metadata">
            <source src="static/videos/candy.mp4" type="video/mp4">
          </video>
          <p class="mt-3">Candy Push. The end-effector pushes small candies into a designated goal area. Object positions are randomized across trials.</p>
        </div>

        <div class="real-world-video-card has-text-centered">
          <video poster="" id="video2" controls muted loop preload="metadata">
            <source src="static/videos/insert.mp4" type="video/mp4">
          </video>
          <p class="mt-3">Peg-in-Hole. The robot must insert a circular peg into a vertical hole using only RGB inputs from scene and wrist cameras, without explicit depth sensing, requiring the policy to infer 3D geometry from indirect cues.</p>
        </div>
		  
        <div class="real-world-video-card has-text-centered">
          <video poster="" id="video3" controls muted loop preload="metadata">
            <source src="static/videos/circle.mp4" type="video/mp4">
          </video>
          <p class="mt-3">Painting Circle. The robot is tasked with tracing a circular-shaped path using a paintbrush.</p>
        </div>

		<div class="real-world-video-card has-text-centered">
          <video poster="" id="video4" controls muted loop preload="metadata">
            <source src="static/videos/heart.mp4" type="video/mp4">
          </video>
          <p class="mt-3">Painting Heart. The robot is tasked with tracing a heart-shaped path using a paintbrush.</p>
        </div>
		  
      </div>
    </div>
  </div>
</section>
<!-- End video row -->


<!-- Video row -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Evaluations on Robomimic Simulation Benchmark</h2>
      <div class="robomimic-video-row">

        <div class="robomimic-video-card has-text-centered">
          <video poster="" id="video1" controls muted loop preload="metadata">
            <source src="static/videos/can.mp4" type="video/mp4">
          </video>
          <p class="mt-3">The Can task involves picking up a can and placing it in the correct bin, testing pick-and-place skills.</p>
        </div>

        <div class="robomimic-video-card has-text-centered">
          <video poster="" id="video2" controls muted loop preload="metadata">
            <source src="static/videos/lift.mp4" type="video/mp4">
          </video>
          <p class="mt-3">The Lift task involves lifting an object, a simple manipulation task that benefits less from large datasets compared to complex tasks.</p>
        </div>

        <div class="robomimic-video-card has-text-centered">
          <video poster="" id="video3" controls muted loop preload="metadata">
            <source src="static/videos/square.mp4" type="video/mp4">
          </video>
          <p class="mt-3">The Square task, also known as Square Nut Assembly, requires fitting a square nut onto a square peg.</p>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End video row -->


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper</h2>
      <p class="has-text-justified">
        Preview our NeurIPS 2025 camera-ready paper.
      </p>
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{wang2025act,
  title={Act to See, See to Act: Diffusion-Driven Perception--Action Interplay for Adaptive Policies},
  author={Wang, Jing and Peng, Weiting and Tang, Jing and Gong, Zeyu and Wang, Xihua and Tao, Bo and Cheng, Li},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
